# -*- coding: utf-8 -*-
"""Popularity Based.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hlaLoAa-nVPUQnDX-7l7ZlFU-iODtCyl
"""

#import basic libraries, read the dataset and create the data frames.
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

#merge books and ratings on the column ‘ISBN’ and merge users and ratings on the column ‘USER-ID’.
books = pd.read_csv('Books.csv')
ratings = pd.read_csv('Ratings.csv')
users = pd.read_csv('Users.csv')
book_ratings = books.merge(ratings ,on = 'ISBN')
user_rating = users.merge(ratings , on = 'User-ID')

#2 new data frames that will have a number of ratings and the average rating for each book - book_num_ratings and book_avg_ratings.
book_num_ratings = book_ratings.groupby('Book-Title')['Book-Rating'].count().reset_index().rename(columns = {'Book-Rating':'Num-Ratings' })
book_avg_ratings = book_ratings.groupby('Book-Title')['Book-Rating'].mean().reset_index().rename(columns = {'Book-Rating':'Avg-Ratings' })
final_rating = book_num_ratings.merge(book_avg_ratings , on = 'Book-Title')

#books with more than 250 ratings sorted in descending order by avg_ratings
popular_books = final_rating[final_rating['Num-Ratings'] > 250].sort_values(by = 'Avg-Ratings'  , ascending= False).reset_index(drop = True).head(50)

popular_books.head(15)

#Recall 

threshold_rating = 4.0

relevant_books = popular_books[popular_books['Avg-Ratings'] >= threshold_rating]

num_relevant = len(final_rating[final_rating['Avg-Ratings'] >= threshold_rating])

num_correct = len(relevant_books)

recall = num_correct / num_relevant
print("Recall = ",recall)

#Coverage 

universe = set(books['Book-Title'])

recommended = set(popular_books['Book-Title'])

coverage = len(recommended) / len(universe)
print("Coverage = ",coverage)

#F1-Score

f1_score = 2 * (precision * recall) / (precision + recall)
print("F1 - Score = ",f1_score)

#Precision
#MAP

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

books = pd.read_csv('Books.csv')
ratings = pd.read_csv('Ratings.csv')
users = pd.read_csv('Users.csv')
book_ratings = books.merge(ratings, on='ISBN')
user_rating = users.merge(ratings, on='User-ID')

train_data, test_data = train_test_split(book_ratings, test_size=0.2, random_state=42)

book_num_ratings = train_data.groupby('Book-Title')['Book-Rating'].count().reset_index().rename(columns={'Book-Rating': 'Num-Ratings'})
book_avg_ratings = train_data.groupby('Book-Title')['Book-Rating'].mean().reset_index().rename(columns={'Book-Rating': 'Avg-Ratings'})
final_rating = book_num_ratings.merge(book_avg_ratings, on='Book-Title')

popular_books = final_rating[final_rating['Num-Ratings'] > 250].sort_values(by='Avg-Ratings', ascending=False).reset_index(drop=True).head(50)

test_data['Recommended'] = test_data['Book-Title'].isin(popular_books['Book-Title']).astype(int)

precision = test_data['Recommended'].mean()

test_data['Cumulative Sum'] = test_data['Recommended'].cumsum()
test_data['Precision@k'] = test_data['Cumulative Sum'] / test_data.index
MAP = test_data.loc[test_data['Recommended'] == 1, 'Precision@k'].mean()

print("Precision: {:.3f}".format(precision))
print("Mean Average Precision (MAP): {:.3f}".format(MAP))